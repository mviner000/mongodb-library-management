<!-- src/views/CSVImportView.vue -->
<script setup lang="ts">
  import { ref, computed, onMounted, onUnmounted, watch } from 'vue'
  import { useRoute } from 'vue-router'
  import { useToast } from '@/components/ui/toast'
  import { Button } from '@/components/ui/button'
  import { storeToRefs } from 'pinia'
  import MongoDBDataTable from '@/components/MongoDBDataTable.vue'
  import { parseCSV } from '@/utils/parseCSV'
  import { useDataTableStore } from '@/store/dataTableStore'
  import { getApiBaseUrl } from '@/utils/api'

  const route = useRoute()
  const { toast } = useToast()
  const dataTableStore = useDataTableStore()
  const { collectionSchema } = storeToRefs(dataTableStore)
  const collectionName = computed(() => route.params.name as string)
  const primaryKey = computed(() => {
    return collectionSchema.value?.primaryKey
  })

  // Debug flag to track pagination behavior
  const DEBUG = true
  const logDebug = (...args: any[]) => {
    if (DEBUG) console.debug('[CSVImport]', ...args)
  }

  // Update data structure to handle pagination
  const dataDisplayMode = ref<'valid' | 'invalid'>('valid')

  const validData = ref({
    data: [] as any[],
    currentPage: 1,
    totalPages: 1,
    isLoading: false,
    total: 0, // Track total record count
  })

  const invalidData = ref({
    data: [] as any[],
    currentPage: 1,
    totalPages: 1,
    isLoading: false,
    total: 0, // Track total record count
  })

  // Update preview data based on mode
  const previewData = computed(() =>
    dataDisplayMode.value === 'valid' ? validData.value.data : invalidData.value.data
  )

  // Compute if there's more data to load
  const hasMore = computed(() => {
    const mode = dataDisplayMode.value
    const result =
      mode === 'valid'
        ? validData.value.currentPage < validData.value.totalPages
        : invalidData.value.currentPage < invalidData.value.totalPages

    logDebug('hasMore computed:', {
      mode,
      currentPage: mode === 'valid' ? validData.value.currentPage : invalidData.value.currentPage,
      totalPages: mode === 'valid' ? validData.value.totalPages : invalidData.value.totalPages,
      result,
    })
    return result
  })

  const fileInput = ref<HTMLInputElement | null>(null)
  const hasImportedData = ref(false)

  // Watch for dataDisplayMode changes to log state transitions
  watch(dataDisplayMode, (newMode, oldMode) => {
    logDebug('Display mode changed:', { from: oldMode, to: newMode, hasMore: hasMore.value })
  })

  // Helper function to normalize document ID regardless of format
  const getNormalizedId = (item: any): string | null => {
    return item?._id?.$oid || (typeof item?._id === 'string' ? item._id : null) || item?.id || null
  }

  // Load more data when button is clicked
  const loadMoreData = async () => {
    const mode = dataDisplayMode.value
    logDebug('loadMoreData triggered for mode:', mode)

    const dataRef = mode === 'valid' ? validData : invalidData
    logDebug('Current pagination state:', {
      page: dataRef.value.currentPage,
      totalPages: dataRef.value.totalPages,
      hasMore: hasMore.value,
      isLoading: dataRef.value.isLoading,
      dataLength: dataRef.value.data.length,
      total: dataRef.value.total,
    })

    if (dataRef.value.isLoading || !hasMore.value) {
      logDebug('Aborting loadMore - loading:', dataRef.value.isLoading, 'hasMore:', hasMore.value)
      return
    }

    dataRef.value.isLoading = true
    try {
      const nextPage = dataRef.value.currentPage + 1
      logDebug('Attempting to load page:', nextPage)

      const validPage = mode === 'valid' ? nextPage : validData.value.currentPage
      const invalidPage = mode === 'invalid' ? nextPage : invalidData.value.currentPage

      const url =
        `${getApiBaseUrl()}/api/csv-temp/${collectionName.value}?` +
        `valid_page=${validPage}&valid_page_size=20&` +
        `invalid_page=${invalidPage}&invalid_page_size=20`

      logDebug('Fetching from URL:', url)

      const response = await fetch(url)
      if (!response.ok) {
        const errorText = await response.text()
        logDebug('Load failed - status:', response.status, 'response:', errorText)
        throw new Error(`Failed to load data: ${response.status}`)
      }

      const responseData = await response.json()
      logDebug('API response structure:', Object.keys(responseData))
      const { valid, invalid } = responseData
      logDebug('API response data:', {
        valid: {
          page: valid.page,
          total: valid.total,
          page_size: valid.page_size,
          dataLength: valid.data?.length,
        },
        invalid: {
          page: invalid.page,
          total: invalid.total,
          page_size: invalid.page_size,
          dataLength: invalid.data?.length,
        },
      })

      if (mode === 'valid') {
        // Check if we actually got new data
        if (!valid.data || valid.data.length === 0) {
          logDebug('Warning: No new valid data received for page', nextPage)
        }

        logDebug('Valid data before update:', validData.value.data.length)

        // Create set of existing IDs with normalized format for robust comparison
        const existingIds = new Set(
          validData.value.data.map(getNormalizedId).filter(Boolean) // Filter out any null IDs
        )

        // Filter out items that already exist by checking normalized IDs
        const newItems = valid.data.filter((item: any) => {
          const normalizedId = getNormalizedId(item)
          return normalizedId && !existingIds.has(normalizedId)
        })

        validData.value.data = [...validData.value.data, ...newItems]
        validData.value.currentPage = valid.page
        validData.value.totalPages = Math.ceil(valid.total / valid.page_size)
        validData.value.total = valid.total
        logDebug('Valid data after update:', {
          length: validData.value.data.length,
          newItemsAdded: newItems.length,
          currentPage: validData.value.currentPage,
          totalPages: validData.value.totalPages,
        })
      } else {
        // Check if we actually got new data
        if (!invalid.data || invalid.data.length === 0) {
          logDebug('Warning: No new invalid data received for page', nextPage)
        }

        logDebug('Invalid data before update:', invalidData.value.data.length)
        // We can't reliably use IDs for invalid data, so check based on error strings
        const getInvalidKey = (item: any) => JSON.stringify(item.errors || [])
        const existingKeys = new Set(invalidData.value.data.map(getInvalidKey))
        const newItems = invalid.data.filter((item: any) => !existingKeys.has(getInvalidKey(item)))

        invalidData.value.data = [...invalidData.value.data, ...newItems]
        invalidData.value.currentPage = invalid.page
        invalidData.value.totalPages = Math.ceil(invalid.total / invalid.page_size)
        invalidData.value.total = invalid.total
        logDebug('Invalid data after update:', {
          length: invalidData.value.data.length,
          newItemsAdded: newItems.length,
          currentPage: invalidData.value.currentPage,
          totalPages: invalidData.value.totalPages,
        })
      }

      logDebug('Updated pagination state:', {
        valid: {
          page: validData.value.currentPage,
          totalPages: validData.value.totalPages,
          hasMore: validData.value.currentPage < validData.value.totalPages,
        },
        invalid: {
          page: invalidData.value.currentPage,
          totalPages: invalidData.value.totalPages,
          hasMore: invalidData.value.currentPage < invalidData.value.totalPages,
        },
      })
    } catch (error: any) {
      console.error('Load error:', error)
      toast({ title: 'Load Error', description: error.message, variant: 'destructive' })
    } finally {
      dataRef.value.isLoading = false
      logDebug('Load completed for mode:', mode)
    }
  }

  const handleFileUpload = async (event: Event) => {
    const file = (event.target as HTMLInputElement).files?.[0]
    if (!file) return

    logDebug('Upload started:', { fileName: file.name, size: file.size })

    try {
      const { data: csvData } = await parseCSV(file)
      logDebug('CSV parsed successfully:', { rowCount: csvData.length })

      const schema = collectionSchema.value.properties || {}

      // Create short name map for field mapping
      const shortNameMap = createShortNameMap(schema)
      logDebug('Field mapping:', shortNameMap)

      // Check for required columns
      const csvHeaders = csvData.length > 0 ? Object.keys(csvData[0]) : []
      logDebug('CSV headers:', csvHeaders)

      const missingRequired = primaryKey.value
        ? !csvHeaders.includes(shortNameMap[primaryKey.value] || primaryKey.value)
          ? [primaryKey.value]
          : []
        : []

      if (missingRequired.length > 0) {
        logDebug('Missing required columns:', missingRequired)
        toast({
          title: 'Invalid CSV Format',
          description: `Missing primary key column: ${missingRequired.join(', ')}`,
          variant: 'destructive',
        })
        return
      }

      // Validate CSV data and split into valid/invalid
      logDebug('Validating CSV data...')
      const { valid: validCsvData, invalid: invalidCsvData } = validateCSVData(csvData, schema)
      logDebug('Validation complete:', {
        validCount: validCsvData.length,
        invalidCount: invalidCsvData.length,
      })

      // Show upload counts in toast
      const validCount = validCsvData.length
      const invalidCount = invalidCsvData.length
      toast({
        title: `CSV Import Results`,
        description: `Valid data: ${validCount}, Invalid data: ${invalidCount}`,
        duration: 5000,
      })

      // Transform valid data for storage
      logDebug('Transforming valid data...')
      const transformedValid = transformCSVData(validCsvData, shortNameMap, collectionSchema.value)
      logDebug('Valid data transformed:', { count: transformedValid.length })

      // Transform invalid data to include all CSV columns
      logDebug('Transforming invalid data...')
      const transformedInvalid = invalidCsvData.map(({ row, errors }) => {
        const transformedRow: Record<string, string> = {}

        // Use CSV headers to include all columns from the original CSV
        csvHeaders.forEach((header) => {
          const value = row[header]
          transformedRow[header] = value !== undefined ? String(value) : ''
        })

        return {
          ...transformedRow,
          errors,
        }
      })
      logDebug('Invalid data transformed:', { count: transformedInvalid.length })

      // Send to backend
      logDebug('Sending data to backend...')
      const response = await fetch(`${getApiBaseUrl()}/api/csv-temp/${collectionName.value}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ valid: transformedValid, invalid: transformedInvalid }),
      })

      if (!response.ok) {
        const errorText = await response.text()
        logDebug('Backend save failed:', { status: response.status, response: errorText })
        throw new Error(`Failed to save temporary data: ${response.status}`)
      }

      logDebug('Data saved to backend successfully')

      // After transforming data, store in appropriate refs
      validData.value.data = transformedValid
      invalidData.value.data = transformedInvalid

      // Set pagination metadata after initial upload
      validData.value.currentPage = 1
      validData.value.totalPages = Math.ceil(transformedValid.length / 20)
      validData.value.total = transformedValid.length

      invalidData.value.currentPage = 1
      invalidData.value.totalPages = Math.ceil(transformedInvalid.length / 20)
      invalidData.value.total = transformedInvalid.length

      logDebug('Pagination state initialized:', {
        valid: {
          page: validData.value.currentPage,
          totalPages: validData.value.totalPages,
          total: validData.value.total,
        },
        invalid: {
          page: invalidData.value.currentPage,
          totalPages: invalidData.value.totalPages,
          total: invalidData.value.total,
        },
      })

      // Use MongoDB-compatible ObjectID for selection
      if (dataDisplayMode.value === 'valid') {
        const ids = transformedValid
          .map((doc: any) => getNormalizedId(doc))
          .filter(Boolean) as string[]
        dataTableStore.selectedRows = new Set(ids)
        logDebug('Selected rows:', { count: ids.length })
      }

      hasImportedData.value = true
      toast({ title: 'CSV Processed', description: 'Data successfully imported' })
    } catch (error: any) {
      // Handle parsing/validation errors
      console.error('Import failed:', error)
      toast({ title: 'Import Failed', description: error.message, variant: 'destructive' })
    }
  }

  // Helper functions
  const validateCSVData = (
    data: any[],
    schema: Record<string, any>
  ): { valid: any[]; invalid: Array<{ row: any; errors: string[] }> } => {
    const valid: any[] = []
    const invalid: Array<{ row: any; errors: string[] }> = []

    data.forEach((row) => {
      const errors: string[] = []

      // Primary key field check
      if (primaryKey.value) {
        const value = row[primaryKey.value]
        if (value === null || value === undefined || value === '') {
          errors.push(`Missing primary key field: ${primaryKey.value}`)
        }
      }

      // Type validation
      Object.entries(row).forEach(([field, value]) => {
        const fieldSchema = schema[field]
        if (!fieldSchema || value === null || value === '') return

        const type = Array.isArray(fieldSchema.bsonType)
          ? fieldSchema.bsonType[0]
          : fieldSchema.bsonType

        if (
          (type === 'number' || type === 'int' || type === 'double' || type === 'long') &&
          isNaN(Number(value))
        ) {
          errors.push(`Invalid number in ${field}`)
        }

        if (
          type === 'bool' &&
          !['true', 'false', '0', '1', 'yes', 'no', ''].includes(String(value).toLowerCase())
        ) {
          errors.push(`Invalid boolean value in ${field}`)
        }

        if (type === 'date' && isNaN(Date.parse(String(value)))) {
          errors.push(`Invalid date in ${field}`)
        }
      })

      if (errors.length > 0) {
        invalid.push({ row, errors })
      } else {
        valid.push(row)
      }
    })

    return { valid, invalid }
  }

  const createShortNameMap = (schema: Record<string, any>) => {
    return Object.keys(schema).reduce(
      (acc, field) => {
        acc[field] = schema[field]?.ui?.short_name || field
        return acc
      },
      {} as Record<string, string>
    )
  }

  const convertValueBySchema = (value: any, fieldSchema: any) => {
    if (value === null || value === undefined || value === '') return null

    const type = Array.isArray(fieldSchema.bsonType)
      ? fieldSchema.bsonType[0]
      : fieldSchema.bsonType

    switch (type) {
      case 'bool':
        if (value === '1' || value === 'yes' || String(value).toLowerCase() === 'true') return true
        if (value === '0' || value === 'no' || String(value).toLowerCase() === 'false') return false
        return null
      case 'int':
      case 'long':
        return parseInt(value, 10)
      case 'double':
      case 'number':
        return parseFloat(value)
      case 'date':
        try {
          return new Date(value).toISOString()
        } catch {
          return null
        }
      default:
        return value
    }
  }

  const transformCSVData = (data: any[], shortNameMap: Record<string, string>, schema: any) => {
    const schemaFields = Object.keys(schema.properties || {})

    return data.map((row) => {
      const transformed: Record<string, any> = {}

      // Map all schema fields
      schemaFields.forEach((field) => {
        // Find the CSV column that corresponds to this schema field
        const csvKey =
          Object.entries(shortNameMap).find(([_, shortName]) => shortName === field)?.[0] || field

        const rawValue = row[csvKey]
        // Convert values based on schema type
        transformed[field] = schema.properties[field]
          ? convertValueBySchema(rawValue, schema.properties[field])
          : rawValue === '' || rawValue === undefined
            ? null
            : rawValue
      })

      // Add MongoDB-compatible ObjectID instead of numeric ID
      transformed._id = { $oid: generateMongoObjectId() }

      // Add timestamps if they're part of the schema
      const now = new Date().toISOString()
      if (schemaFields.includes('created_at')) {
        transformed.created_at = transformed.created_at || now
      }

      if (schemaFields.includes('updated_at')) {
        transformed.updated_at = transformed.updated_at || now
      }

      return transformed
    })
  }

  // Function to generate MongoDB-compatible ObjectIDs
  function generateMongoObjectId(): string {
    const timestamp = Math.floor(new Date().getTime() / 1000)
      .toString(16)
      .padStart(8, '0')
    const machineId = randomHexString(6)
    const processId = randomHexString(4)
    const counter = randomHexString(6)

    return timestamp + machineId + processId + counter
  }

  // Helper function to generate random hex strings of specified length
  function randomHexString(length: number): string {
    let result = ''
    const characters = '0123456789abcdef'
    for (let i = 0; i < length; i++) {
      result += characters.charAt(Math.floor(Math.random() * characters.length))
    }
    return result
  }

  onMounted(async () => {
    logDebug('Component mounted, fetching initial data')
    try {
      // Load from paginated backend
      const response = await fetch(
        `${getApiBaseUrl()}/api/csv-temp/${collectionName.value}?` +
          `valid_page=1&valid_page_size=20&invalid_page=1&invalid_page_size=20`
      )

      if (response.ok) {
        const { valid, invalid } = await response.json()
        logDebug('Initial data loaded:', {
          valid: { count: valid.data?.length, total: valid.total },
          invalid: { count: invalid.data?.length, total: invalid.total },
        })

        validData.value.data = valid.data || []
        validData.value.currentPage = valid.page || 1
        validData.value.totalPages = Math.ceil(valid.total / valid.page_size) || 1
        validData.value.total = valid.total || 0

        invalidData.value.data = invalid.data || []
        invalidData.value.currentPage = invalid.page || 1
        invalidData.value.totalPages = Math.ceil(invalid.total / invalid.page_size) || 1
        invalidData.value.total = invalid.total || 0

        dataDisplayMode.value = 'valid'
        hasImportedData.value = validData.value.data.length > 0 || invalidData.value.data.length > 0

        logDebug('Pagination state initialized:', {
          valid: {
            page: validData.value.currentPage,
            totalPages: validData.value.totalPages,
            hasMore: validData.value.currentPage < validData.value.totalPages,
            total: validData.value.total,
          },
          invalid: {
            page: invalidData.value.currentPage,
            totalPages: invalidData.value.totalPages,
            hasMore: invalidData.value.currentPage < invalidData.value.totalPages,
            total: invalidData.value.total,
          },
        })

        // Auto-select all rows when loading saved data - use normalized ID format
        if (validData.value.data.length > 0) {
          const ids = validData.value.data.map(getNormalizedId).filter(Boolean) as string[]
          dataTableStore.selectedRows = new Set(ids)
          logDebug('Selected rows initialized:', { count: ids.length })
        }
      } else {
        const errorText = await response.text()
        logDebug('Initial data load failed:', { status: response.status, response: errorText })
      }
    } catch (error) {
      console.error('Error loading CSV data:', error)
    }
  })

  const triggerFileSelect = () => {
    logDebug('File select triggered')
    fileInput.value?.click()
  }

  const resetImport = async () => {
    logDebug('Import reset requested')
    try {
      // Delete from backend
      await fetch(`${getApiBaseUrl()}/api/csv-temp/${collectionName.value}`, { method: 'DELETE' })
      logDebug('Backend data deleted')

      validData.value = {
        data: [],
        currentPage: 1,
        totalPages: 1,
        isLoading: false,
        total: 0,
      }

      invalidData.value = {
        data: [],
        currentPage: 1,
        totalPages: 1,
        isLoading: false,
        total: 0,
      }

      hasImportedData.value = false
      // Clear selected rows when resetting
      dataTableStore.selectedRows = new Set()
      if (fileInput.value) {
        fileInput.value.value = ''
      }
      logDebug('Import reset complete')
      toast({ title: 'Reset Complete', description: 'Temporary data cleared' })
    } catch (error: any) {
      console.error('Reset failed:', error)
      toast({ title: 'Reset Failed', description: error.message, variant: 'destructive' })
    }
  }

  onUnmounted(() => {
    logDebug('Component unmounting, clearing selected rows')
    // Clear selected rows when component is unmounted
    dataTableStore.selectedRows = new Set()
  })
</script>

<template>
  <!-- Conditional rendering based on whether data is imported -->
  <div
    v-if="!hasImportedData"
    class="file-import-container"
  >
    <input
      ref="fileInput"
      type="file"
      accept=".csv"
      class="hidden"
      @change="handleFileUpload"
    />

    <div class="excel-inspired-import">
      <div class="import-header">
        <h2>Import CSV to {{ collectionName }}</h2>
        <p class="import-subtitle">Add data to your collection from a CSV file</p>
      </div>

      <div
        class="import-dropzone"
        @click="triggerFileSelect"
      >
        <div class="dropzone-content">
          <div class="excel-icon">
            <svg
              xmlns="http://www.w3.org/2000/svg"
              viewBox="0 0 24 24"
              width="48"
              height="48"
            >
              <path
                fill="none"
                d="M0 0h24v24H0z"
              />
              <path
                d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8l-6-6zm1 5h-5V4h1v2h4v1zm-8 5h3v-1H7v1zm0 2h3v-1H7v1zm0 2h8v-1H7v1zm8-6v-1H7v1h8z"
                fill="#107C41"
              />
            </svg>
          </div>
          <p class="dropzone-text">
            <span class="font-bold">Select a CSV file</span> or drag and drop it here
          </p>
          <div class="mt-4">
            <Button class="primary-button">Browse Files</Button>
          </div>
          <p class="text-xs text-gray-500 mt-2">
            Make sure your CSV has headers matching the collection schema
          </p>
        </div>
      </div>

      <div
        v-if="fileInput?.files?.[0]"
        class="selected-file"
      >
        <span class="file-name">{{ fileInput.files[0].name }}</span>
        <span class="file-size">{{ (fileInput.files[0].size / 1024).toFixed(1) }} KB</span>
      </div>
    </div>
  </div>

  <!-- Table view when data is imported -->
  <div v-else>
    <div class="csv-import-container">
      <!-- Debug information -->
      <div
        v-if="DEBUG"
        class="debug-info bg-gray-100 p-2 mb-4 text-xs font-mono overflow-auto"
      >
        <details>
          <summary class="font-bold cursor-pointer">Debug Info ({{ dataDisplayMode }})</summary>
          <pre>
Mode: {{ dataDisplayMode }}
Valid data: {{ validData.data.length }} / {{ validData.total }} items (page {{
              validData.currentPage
            }}/{{ validData.totalPages }})
Invalid data: {{ invalidData.data.length }} / {{ invalidData.total }} items (page {{
              invalidData.currentPage
            }}/{{ invalidData.totalPages }})
Has more: {{ hasMore }}
          </pre>
        </details>
      </div>

      <MongoDBDataTable
        :preview-data="previewData"
        :data-display-mode="dataDisplayMode"
        @update:data-display-mode="dataDisplayMode = $event"
        preview-mode
      />

      <!-- Show More Button -->
      <div
        v-if="hasMore"
        class="show-more-container"
      >
        <Button
          :disabled="dataDisplayMode === 'valid' ? validData.isLoading : invalidData.isLoading"
          @click="loadMoreData"
          class="show-more-button"
        >
          <template
            v-if="dataDisplayMode === 'valid' ? validData.isLoading : invalidData.isLoading"
          >
            <svg
              class="animate-spin -ml-1 mr-2 h-4 w-4"
              xmlns="http://www.w3.org/2000/svg"
              fill="none"
              viewBox="0 0 24 24"
            >
              <circle
                class="opacity-25"
                cx="12"
                cy="12"
                r="10"
                stroke="currentColor"
                stroke-width="4"
              ></circle>
              <path
                class="opacity-75"
                fill="currentColor"
                d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"
              ></path>
            </svg>
            Loading...
          </template>
          <template v-else>
            <svg
              xmlns="http://www.w3.org/2000/svg"
              width="16"
              height="16"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
              stroke-linecap="round"
              stroke-linejoin="round"
              class="mr-2"
            >
              <polyline points="7 13 12 18 17 13"></polyline>
              <polyline points="7 6 12 11 17 6"></polyline>
            </svg>
            Show More
          </template>
        </Button>

        <div class="text-xs text-gray-500 mt-2 text-center">
          Showing
          {{ dataDisplayMode === 'valid' ? validData.data.length : invalidData.data.length }} of
          {{ dataDisplayMode === 'valid' ? validData.total : invalidData.total }}
          {{ dataDisplayMode === 'valid' ? 'valid' : 'invalid' }} records
        </div>
      </div>
    </div>
  </div>

  <!-- Reset button - completely separate from the content flow -->
  <button
    v-if="hasImportedData"
    @click="resetImport"
    class="fixed bottom-4 left-4 z-40 flex items-center px-4 py-2 text-sm font-medium text-white bg-red-600 hover:bg-red-700 rounded-full shadow-lg transition-colors duration-200 ease-in-out"
  >
    <svg
      xmlns="http://www.w3.org/2000/svg"
      width="16"
      height="16"
      viewBox="0 0 24 24"
      fill="none"
      stroke="currentColor"
      stroke-width="2"
      stroke-linecap="round"
      stroke-linejoin="round"
      class="mr-2"
    >
      <path d="M21 12a9 9 0 0 0-9-9 9.75 9.75 0 0 0-6.74 2.74L3 8" />
      <path d="M3 3v5h5" />
      <path d="M3 12a9 9 0 0 0 9 9 9.75 9.75 0 0 0 6.74-2.74L21 16" />
      <path d="M16 16h5v5" />
    </svg>
    Reset
  </button>
</template>

<style scoped>
  .csv-import-container {
    @apply px-4 max-w-full overflow-x-auto;
  }

  .file-import-container {
    @apply p-6 max-w-3xl mx-auto;
  }

  .excel-inspired-import {
    @apply bg-white rounded-lg shadow-md border border-gray-200;
  }

  .import-header {
    @apply p-4 border-b border-gray-200;
  }

  .import-header h2 {
    @apply text-xl font-semibold text-gray-800;
  }

  .import-subtitle {
    @apply text-sm text-gray-500 mt-1;
  }

  .import-dropzone {
    @apply p-8 flex flex-col items-center justify-center cursor-pointer transition-all duration-200 hover:bg-gray-50;
  }

  .dropzone-content {
    @apply flex flex-col items-center justify-center gap-3;
  }

  .excel-icon {
    @apply mb-3;
  }

  .dropzone-text {
    @apply text-gray-600 text-center;
  }

  .primary-button {
    @apply bg-green-600 hover:bg-green-700 text-white;
  }

  .selected-file {
    @apply p-4 border-t border-gray-200 flex justify-between items-center text-sm;
  }

  .file-name {
    @apply font-medium text-gray-700;
  }

  .file-size {
    @apply text-gray-500;
  }

  .debug-info {
    @apply rounded border border-gray-300;
  }
</style>
